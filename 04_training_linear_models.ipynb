{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 4 – Training Linear Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 4._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_linear_models\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using the Normal Equation\n",
    "\n",
    "A __linear__ model makes a predictrion by simply computing a weigted sum of the input features, plus a constant called the ___bias___ term (also called the ___intercept___ term).\n",
    "\n",
    "$\\displaystyle \\hat{y} = \\theta_0 + \\theta_1 x_1 +\\theta_2 x_2 + \\dots + \\theta_n x_n$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $\\hat y$ is the predicted value\n",
    "- $n$ is the number of features\n",
    "- $x_i$ is the $i^{th}$ feature value\n",
    "- $\\theta_j$ is the $j^{th}$ model parameter (including the bias term $\\theta_0$ and the feature weights $\\theta_1, \\theta_2, \\dots, \\theta_n$)\n",
    "\n",
    "\n",
    "This can be written much more concisely using a vectorized form:\n",
    "\n",
    "$\\displaystyle \\hat{y} = h_{\\theta}(x) = \\theta. x$\n",
    "\n",
    "\n",
    "In this equation:\n",
    "- $\\theta$ is the model's ___parameter vector___, containing the bias term $\\theta_0$ andd the feature weights $\\theta_1$ to $\\theta_n$\n",
    "- $x$ is the instance's ___feature vector___, containing $x_0$ to $x_n$, with $x_0$ always equal to 1\n",
    "- $\\theta . x$ is the dot product of the vectors $\\theta$ and $x$, which is of course equal to $\\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$\n",
    "- $h_0$ is the hypothesis function, using the model parameters $\\theta$\n",
    "\n",
    "In Machine Learning, vectors are often represented as _column vectors_, which are 2D arrays with a single column. If $\\theta$ and $x$ are column vectors, then the prediction is $\\hat{y} = \\theta^Tx$, where $\\theta^T$ is the _transpose_ of $\\theta$ (a row vector instead of a column vector) and $\\theta^Tx$ is a matrix multiplication of $\\theta^T$ and $x$. It is the same prediction, except that it is now represented as a single cell matrix rather than a scalar value.\n",
    "\n",
    "__Training a Linear Regression model__<br>\n",
    "To train a Linear Regression model, we need to find the value of$\\theta$ that minimizes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "The MSE of the Linear Regression hypothesis $h_0$ on a training set $X$ is calculated using the following equation:\n",
    "\n",
    "$\\displaystyle MSE(X, h_\\theta) = \\frac{1}{m}\\sum_{i=1}^m \\left(\\theta^Tx^{(i)} - y^{(i)}\\right)^2$\n",
    "\n",
    "We will write $MSE(X, h_\\theta)$ as $MSE(\\theta)$ to simplify notations.\n",
    "\n",
    "## The Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a _closed-form solution_ - in other words a  mathematical equation that gives the result directly. This is called the ___Normal Equation___:\n",
    "\n",
    "$\\displaystyle \\hat{\\theta} = (X^TX)^{-1}\\;X^T\\;y$\n",
    "\n",
    "In this equation:\n",
    "-$\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function\n",
    "- $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$ \n",
    "\n",
    "Lets generate some linear-looking data to test this equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure generated_data_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"generated_data_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute $\\hat{\\theta}$ using the Normal Equation. The __`inv()`__ funtion from Numpy's linear algebra module (__`np.linalg`__) computes the inverse of a matrix, and the __`dot()`__ method is used for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that we used to generate the data is $y = 4 + 3x_1 +\\text{Gaussian noise}$. Lets see what the equation found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had hoped for $\\theta_0 = 4$ and $\\theta_1 = 3$, but the noise made it impossible to recover the exact parameters of rthe original function. Now we can make predictions using $\\hat{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the book actually corresponds to the following code, with a legend and axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure linear_model_predictions_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"linear_model_predictions_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Regression using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21509616]), array([[2.77011339]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\"), which you could call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes $\\mathbf{X}^+\\mathbf{y}$, where $\\mathbf{X}^{+}$ is the _pseudoinverse_ of $\\mathbf{X}$ (specifically the Moore-Penrose inverse). You can use `np.linalg.pinv()` to compute the pseudoinverse directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinverse is computed using a standard matrix factorization technique called _Singular Value Decomposition_ (SVD) that can decompose the training set matrix $X$ into the matrix multiplication of three matrices $U\\;\\Sigma\\;V^T$. The pseudoinverse is computed as $X^+ = V\\Sigma^+U^T$. To compute the matrix $\\Sigma^+$, the algorithm takes $\\Sigma$ and sets to zero all values smaller than a tiny threshold value, then it replaces all the nonzero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal equation, plus it handles edge cases nicely: the Normal Equation may not work if the matrix $X^TX$ is not invertible (i.e., singular), such as if $m < n$ or if some features are redundant, but the psedoinverse is always defined.\n",
    "\n",
    "\n",
    "The _computational complexity_ of inverting a matrix with $n$ features is typically about $O(n^{2.4})$ to $O(n^{3})$, depending on the implementation. So, if we double the number features, we multiply the computation time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.  \n",
    "\n",
    "The SVD approach used by Scikit-Learn's __`LinearRegression`__ class is about $O(n^{2})$. If we double the number of features, we multiply the computation timer by roughly 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_b.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = []\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "\n",
    "save_fig(\"gradient_descent_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:                    # not shown in the book\n",
    "            y_predict = X_new_b.dot(theta)           # not shown\n",
    "            style = \"b-\" if i > 0 else \"r--\"         # not shown\n",
    "            plt.plot(X_new, y_predict, style)        # not shown\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 # not shown\n",
    "\n",
    "plt.plot(X, y, \"b.\")                                 # not shown\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     # not shown\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           # not shown\n",
    "plt.axis([0, 2, 0, 15])                              # not shown\n",
    "save_fig(\"sgd_plot\")                                 # not shown\n",
    "plt.show()                                           # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
    "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
    "save_fig(\"gradient_descent_paths_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "save_fig(\"quadratic_data_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "save_fig(\"quadratic_predictions_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "save_fig(\"high_degree_polynomials_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
    "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
    "save_fig(\"underfitting_learning_curves_plot\")   # not shown\n",
    "plt.show()                                      # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])           # not shown\n",
    "save_fig(\"learning_curves_plot\")  # not shown\n",
    "plt.show()                        # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 20\n",
    "X = 3 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "X_new = np.linspace(0, 3, 100).reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
    "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
    "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "        if polynomial:\n",
    "            model = Pipeline([\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "                    (\"std_scaler\", StandardScaler()),\n",
    "                    (\"regul_reg\", model),\n",
    "                ])\n",
    "        model.fit(X, y)\n",
    "        y_new_regul = model.predict(X_new)\n",
    "        lw = 2 if alpha > 0 else 1\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 3, 0, 4])\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(122)\n",
    "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
    "\n",
    "save_fig(\"ridge_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: to be future-proof, we set `max_iter=1000` and `tol=1e-3` because these will be the default values in Scikit-Learn 0.21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(122)\n",
    "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n",
    "\n",
    "save_fig(\"lasso_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(sgd_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
    "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
    "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "save_fig(\"early_stopping_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
    "\n",
    "t1s = np.linspace(t1a, t1b, 500)\n",
    "t2s = np.linspace(t2a, t2b, 500)\n",
    "t1, t2 = np.meshgrid(t1s, t2s)\n",
    "T = np.c_[t1.ravel(), t2.ravel()]\n",
    "Xr = np.array([[1, 1], [1, -1], [1, 0.5]])\n",
    "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
    "\n",
    "J = (1/len(Xr) * np.sum((T.dot(Xr.T) - yr.T)**2, axis=1)).reshape(t1.shape)\n",
    "\n",
    "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
    "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
    "\n",
    "t_min_idx = np.unravel_index(np.argmin(J), J.shape)\n",
    "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
    "\n",
    "t_init = np.array([[0.25], [-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd_path(theta, X, y, l1, l2, core = 1, eta = 0.05, n_iterations = 200):\n",
    "    path = [theta]\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = core * 2/len(X) * X.T.dot(X.dot(theta) - y) + l1 * np.sign(theta) + l2 * theta\n",
    "        theta = theta - eta * gradients\n",
    "        path.append(theta)\n",
    "    return np.array(path)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n",
    "for i, N, l1, l2, title in ((0, N1, 2., 0, \"Lasso\"), (1, N2, 0,  2., \"Ridge\")):\n",
    "    JR = J + l1 * N1 + l2 * 0.5 * N2**2\n",
    "    \n",
    "    tr_min_idx = np.unravel_index(np.argmin(JR), JR.shape)\n",
    "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
    "\n",
    "    levelsJ=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(J) - np.min(J)) + np.min(J)\n",
    "    levelsJR=(np.exp(np.linspace(0, 1, 20)) - 1) * (np.max(JR) - np.min(JR)) + np.min(JR)\n",
    "    levelsN=np.linspace(0, np.max(N), 10)\n",
    "    \n",
    "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
    "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
    "    path_N = bgd_path(np.array([[2.0], [0.5]]), Xr, yr, np.sign(l1)/3, np.sign(l2), core=0)\n",
    "\n",
    "    ax = axes[i, 0]\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.contourf(t1, t2, N / 2., levels=levelsN)\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.set_title(r\"$\\ell_{}$ penalty\".format(i + 1), fontsize=16)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$\\theta_2$\", fontsize=16, rotation=0)\n",
    "\n",
    "    ax = axes[i, 1]\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
    "    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.plot(t1r_min, t2r_min, \"rs\")\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\", fontsize=16)\n",
    "\n",
    "save_fig(\"lasso_vs_ridge_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "save_fig(\"logistic_function_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: To be future-proof we set `solver=\"lbfgs\"` since this will be the default value in Scikit-Learn 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the book actually is actually a bit fancier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "save_fig(\"logistic_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "save_fig(\"logistic_regression_contour_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "save_fig(\"softmax_regression_contour_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Batch Gradient Descent with early stopping for Softmax Regression\n",
    "(without using Scikit-Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the data. We will just reuse the Iris dataset we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the bias term for every instance ($x_0 = 1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's set the random seed so the output of this exercise solution is reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn's `train_test_split()` function, but the point of this exercise is to try understand the algorithms by implementing them manually. So here is one possible implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). Let's write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function on the first 10 instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_one_hot(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, so let's create the target class probabilities matrix for the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_valid_one_hot = to_one_hot(y_valid)\n",
    "Y_test_one_hot = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the Softmax function. Recall that it is defined by the following equation:\n",
    "\n",
    "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to start training. Let's define the number of inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
    "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here comes the hardest part: training! Theoretically, it's simple: it's just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it's easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it's working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won't have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what's going on under the hood.\n",
    "\n",
    "So the equations we will need are the cost function:\n",
    "\n",
    "$J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "And the equation for the gradients:\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
    "\n",
    "Note that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\\epsilon$ to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting `nan` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! The Softmax model is trained. Let's look at the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions for the validation set and check the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this model looks pretty good. For the sake of the exercise, let's add a bit of $\\ell_2$ regularization. The following training code is similar to the one above, but the loss now has an additional $\\ell_2$ penalty, and the gradients have the proper additional term (note that we don't regularize the first element of `Theta` since this corresponds to the bias term). Also, let's try increasing the learning rate `eta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1  # regularization hyperparameter\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the additional $\\ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, perfect accuracy! We probably just got lucky with this validation set, but still, it's pleasant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 \n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1  # regularization hyperparameter\n",
    "best_loss = np.infty\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients\n",
    "\n",
    "    logits = X_valid.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still perfect, but faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the model's predictions on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n",
    "\n",
    "logits = X_new_with_bias.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "zz1 = Y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's measure the final model's accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X_test.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our perfect model turns out to have slight imperfections. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
